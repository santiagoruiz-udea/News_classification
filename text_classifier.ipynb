{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-classifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook probar diferentes clasificadores"
      ],
      "metadata": {
        "id": "RWjooeTYAnTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se pretende explorar diferentes clasificadores variando los hyperparametros, escogiendo el mejor resultado."
      ],
      "metadata": {
        "id": "KcIrGa9gAunO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Importación librerias"
      ],
      "metadata": {
        "id": "EjDjZObJA64d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AB-3fopyAdC2"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.naive_bayes import  MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier, Perceptron, LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "import re\n",
        "import time\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('popular')"
      ],
      "metadata": {
        "id": "U-SGSteDFvoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
        "from nltk import word_tokenize"
      ],
      "metadata": {
        "id": "sjhvhB_wHYWO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news = fetch_20newsgroups(subset=\"all\",remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "print(\"Número de articulos: {}\".format(len(news.data)))\n",
        "print(\"Número de categorias: {}\".format(len(news.target_names)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKRgkTfcBOcI",
        "outputId": "90e4d544-e96a-492d-c7e8-e3ec19a5dd77"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de articulos: 18846\n",
            "Número de categorias: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Definción función de entrenamiento  "
      ],
      "metadata": {
        "id": "MmVicDeMB-q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(classifier, X, Y):\n",
        "  start = time.time()\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
        "  classifier.fit(X_train, Y_train)\n",
        "  end = time.time()\n",
        "  score = classifier.score(X_test, Y_test)\n",
        "  print(\"Test accuracy: {:.2f}% - Time duration: {:.2f} s\".format(score * 100, (end-start)))\n",
        "  return classifier"
      ],
      "metadata": {
        "id": "PjNpzoH2B-Hx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Preprocesado"
      ],
      "metadata": {
        "id": "7PXKQREVHiwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1. Limpiar el texto"
      ],
      "metadata": {
        "id": "rkut_9knpwva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(news):\n",
        "\n",
        "  df = pd.DataFrame(news.data, columns=['text'])\n",
        "  df[\"categories\"] = [news.target_names[i] for i in news.target]\n",
        "  df[\"labels\"] = [i for i in news.target]\n",
        "\n",
        "  # Remove multiples whitspaces characters\n",
        "  df[\"text\"] = [re.sub('\\s+', ' ', sent) for sent in df[\"text\"]]\n",
        "  # Remove whitspace star and end text\n",
        "  df['text'] = df['text'].str.strip()\n",
        "  # Remove Emails\n",
        "  df[\"text\"] = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in df[\"text\"]]\n",
        "  # Remove distracting single quotes\n",
        "  df[\"text\"] = [re.sub(\"\\'\", \"\", sent) for sent in df[\"text\"]]\n",
        "  # Remove any rows with empty fields\n",
        "  df = df.replace('', np.NaN).dropna()\n",
        "  # Drop duplicates\n",
        "  df = df.drop_duplicates(subset='text')\n",
        "  # Remove number\n",
        "  df['text'] = [re.sub(r'\\[[0-9]*\\]', ' ', text) for text in df['text']]\n",
        "  # lower\n",
        "  df[\"text\"] = [text.lower() for text in df[\"text\"]]\n",
        "  return df"
      ],
      "metadata": {
        "id": "RPULiDtNpvJ3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2. Stemming"
      ],
      "metadata": {
        "id": "NZd4fKtwp26b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stemming_tokenizer(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(w) for w in word_tokenize(text)]"
      ],
      "metadata": {
        "id": "FGu3xG88Hh96"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3. Lemmatization"
      ],
      "metadata": {
        "id": "Y96NX05wgR1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the function for lemmatization\n",
        "def lemmatizer_tokenizer(text):\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    return [wordnet_lemmatizer.lemmatize(w) for w in word_tokenize(text)]"
      ],
      "metadata": {
        "id": "RKGi6rnBgRUR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = clean_text(news)\n",
        "data = df[\"text\"].values\n",
        "target = df[\"labels\"].values"
      ],
      "metadata": {
        "id": "Fet__Q76qGqX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Construcción de clasificadores - TfidfVectorizer"
      ],
      "metadata": {
        "id": "IBdlxOaPEPD6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1. Multinominal Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "iF8aicupEXEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for alpha in [1, 0.1, 0.01, 0.001, 0.0001]:\n",
        "  NBclf = Pipeline(\n",
        "      [('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english')+ list(string.punctuation))),\n",
        "      ('classifier', MultinomialNB(alpha=alpha))]\n",
        "  )\n",
        "\n",
        "  train_fn(NBclf, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JesfWH5YBxKc",
        "outputId": "4436501b-0123-4ef3-c843-cca7ffb2c7bc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 73.62% - Time duration: 2.60 s\n",
            "Test accuracy: 78.14% - Time duration: 2.69 s\n",
            "Test accuracy: 78.91% - Time duration: 2.66 s\n",
            "Test accuracy: 78.17% - Time duration: 2.62 s\n",
            "Test accuracy: 75.92% - Time duration: 2.65 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se escoge el mejor desempeño y se realiza prueba eliminado signo de puntuación y stemming."
      ],
      "metadata": {
        "id": "0Y73u9n6IOzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NBclf = Pipeline(\n",
        "    [('vectorizer', TfidfVectorizer(tokenizer=stemming_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))),\n",
        "    ('classifier', MultinomialNB(alpha=0.01))]\n",
        ")\n",
        "train_fn(NBclf, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brZd2_m6FHQO",
        "outputId": "9016342f-0fd0-48a7-8451-067f0a085c78"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 79.05% - Time duration: 80.74 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function stemming_tokenizer at 0x7f8727181320>)),\n",
              "                ('classifier', MultinomialNB(alpha=0.01))])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NBclf = Pipeline(\n",
        "    [('vectorizer', TfidfVectorizer(tokenizer=lemmatizer_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))),\n",
        "    ('classifier', MultinomialNB(alpha=0.01))]\n",
        ")\n",
        "train_fn(NBclf, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nq6ufPteg9ND",
        "outputId": "bfdd6a3b-3059-4c3a-d165-5c104711742b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 78.88% - Time duration: 40.30 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function lemmatizer_tokenizer at 0x7f872706b710>)),\n",
              "                ('classifier', MultinomialNB(alpha=0.01))])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2. Support Vector Classification con stochatic gradient descent"
      ],
      "metadata": {
        "id": "f05ulzFmJHKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for alpha in [1, 0.1, 0.01, 0.001, 0.0001, 0.00001]:  \n",
        "  SVMclf1 = Pipeline(\n",
        "      [('vectorizer',TfidfVectorizer(stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "      ('classifier', SGDClassifier(alpha=alpha))]\n",
        "  )\n",
        "\n",
        "  train_fn(SVMclf1, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJZfEqcSIuYK",
        "outputId": "70f07a9a-0357-418c-b70d-124970473801"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 5.45% - Time duration: 5.47 s\n",
            "Test accuracy: 50.31% - Time duration: 6.44 s\n",
            "Test accuracy: 75.32% - Time duration: 3.59 s\n",
            "Test accuracy: 75.62% - Time duration: 3.60 s\n",
            "Test accuracy: 77.70% - Time duration: 4.15 s\n",
            "Test accuracy: 75.79% - Time duration: 4.24 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se escoge el mejor desempeño y se realiza prueba eliminado signo de puntuación y stemming."
      ],
      "metadata": {
        "id": "ThuTi9rHNYPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SVMclf1 = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(tokenizer=stemming_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', SGDClassifier(alpha=0.0001))]\n",
        "  )\n",
        "\n",
        "train_fn(SVMclf1, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcpMNTR0J0Bp",
        "outputId": "51f3f809-e974-41d1-908c-423489509ab8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 76.09% - Time duration: 82.32 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function stemming_tokenizer at 0x7f8727181320>)),\n",
              "                ('classifier', SGDClassifier())])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SVMclf1 = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(tokenizer=lemmatizer_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', SGDClassifier(alpha=0.0001))]\n",
        "  )\n",
        "\n",
        "train_fn(SVMclf1, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgCX-FKqhgAd",
        "outputId": "dc02c0f0-fff9-4959-9c62-cf707b6631e6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 75.73% - Time duration: 41.58 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function lemmatizer_tokenizer at 0x7f872706b710>)),\n",
              "                ('classifier', SGDClassifier())])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.3. Support Vector Classification con liner SVC"
      ],
      "metadata": {
        "id": "CSxjUK-tNaK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SVMclf2 = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', LinearSVC())]\n",
        ")\n",
        "\n",
        "train_fn(SVMclf2, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0PWzhVGME42",
        "outputId": "d1bdaf6b-2d55-44df-d592-7c316518e35a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 79.02% - Time duration: 4.85 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
              "                ('classifier', LinearSVC())])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con stemming."
      ],
      "metadata": {
        "id": "vvQiI8ukOY5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SVMclf2 = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(tokenizer=stemming_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', LinearSVC())]\n",
        ")\n",
        "\n",
        "train_fn(SVMclf2, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTAYkT3FOYa1",
        "outputId": "29be9623-87a5-4971-b300-4aa72e305d85"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 77.35% - Time duration: 83.61 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function stemming_tokenizer at 0x7f8727181320>)),\n",
              "                ('classifier', LinearSVC())])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SVMclf2 = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(tokenizer=lemmatizer_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', LinearSVC())]\n",
        ")\n",
        "\n",
        "train_fn(SVMclf2, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7E-WG3tMhrpb",
        "outputId": "37e53a39-ec64-4007-fb1c-100366e5858e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 77.46% - Time duration: 44.04 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function lemmatizer_tokenizer at 0x7f872706b710>)),\n",
              "                ('classifier', LinearSVC())])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.4. Perceptron"
      ],
      "metadata": {
        "id": "JVouH3sqRwni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for alpha in [1, 0.1, 0.01, 0.001, 0.0001, 0.00001]:  \n",
        "  perclf = Pipeline(\n",
        "      [('vectorizer',TfidfVectorizer(stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "      ('classifier', Perceptron(alpha=alpha))]\n",
        "  )\n",
        "\n",
        "  train_fn(perclf, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AM188sjFOMK6",
        "outputId": "95f75d02-cfed-4bcf-d94e-7214111f54ab"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 72.45% - Time duration: 3.76 s\n",
            "Test accuracy: 71.84% - Time duration: 3.79 s\n",
            "Test accuracy: 72.47% - Time duration: 3.68 s\n",
            "Test accuracy: 71.49% - Time duration: 3.71 s\n",
            "Test accuracy: 70.75% - Time duration: 3.71 s\n",
            "Test accuracy: 72.34% - Time duration: 3.68 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perclf = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(tokenizer=stemming_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', Perceptron(alpha=0.00001))]\n",
        ")\n",
        "\n",
        "train_fn(perclf, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ijh0cvY6SE0w",
        "outputId": "ca390bdf-66f0-4d21-e1bd-0b4f912c4049"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 70.36% - Time duration: 80.86 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function stemming_tokenizer at 0x7f8727181320>)),\n",
              "                ('classifier', Perceptron(alpha=1e-05))])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perclf = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(tokenizer=lemmatizer_tokenizer, stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', Perceptron(alpha=0.00001))]\n",
        ")\n",
        "\n",
        "train_fn(perclf, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8fo_KbRh-vg",
        "outputId": "9840a69f-10b9-491a-8867-e805639f64b0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 72.06% - Time duration: 41.51 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function lemmatizer_tokenizer at 0x7f872706b710>)),\n",
              "                ('classifier', Perceptron(alpha=1e-05))])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.5. Logistic Regression"
      ],
      "metadata": {
        "id": "ZZU9cUHku73C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perclf = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(tokenizer=stemming_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', LogisticRegression(max_iter=10000))]\n",
        ")\n",
        "\n",
        "train_fn(perclf, data, target)"
      ],
      "metadata": {
        "id": "kGG3e9GxSVYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97eca174-0212-4bf6-e02b-e90ab302bb59"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 73.71% - Time duration: 154.60 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function stemming_tokenizer at 0x7f8727181320>)),\n",
              "                ('classifier', LogisticRegression(max_iter=10000))])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perclf = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(tokenizer=lemmatizer_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', LogisticRegression(max_iter=10000))]\n",
        ")\n",
        "\n",
        "train_fn(perclf, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfHFU_T1jpEW",
        "outputId": "133c9040-4a0f-4125-cbdd-fb89fd0e1541"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 73.40% - Time duration: 120.85 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function lemmatizer_tokenizer at 0x7f872706b710>)),\n",
              "                ('classifier', LogisticRegression(max_iter=10000))])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Construcción de clasificadores - CountVectorizer"
      ],
      "metadata": {
        "id": "a7pkA37NwMGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.1. Multinominal Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "d-LbcMunwuNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for alpha in [1, 0.1, 0.01, 0.001, 0.0001]:\n",
        "  NBclf = Pipeline(\n",
        "      [('vectorizer', CountVectorizer(stop_words=stopwords.words('english') + list(string.punctuation))),\n",
        "      ('classifier', MultinomialNB(alpha=alpha))]\n",
        "  )\n",
        "\n",
        "  train_fn(NBclf, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljZhvSbVvnZo",
        "outputId": "e3264553-1f45-4599-ff15-860bed4c7986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 68.06% - Time duration: 2.77 s\n",
            "Test accuracy: 73.46% - Time duration: 2.78 s\n",
            "Test accuracy: 74.36% - Time duration: 2.80 s\n",
            "Test accuracy: 72.34% - Time duration: 2.95 s\n",
            "Test accuracy: 71.73% - Time duration: 2.86 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NBclf = Pipeline(\n",
        "    [('vectorizer', CountVectorizer(tokenizer=stemming_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))),\n",
        "    ('classifier', MultinomialNB(alpha=0.01))]\n",
        ")\n",
        "train_fn(NBclf, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXs-YGggw2Eb",
        "outputId": "d2dcce86-a04f-4a04-e130-51fb29e76f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 73.19% - Time duration: 83.91 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 CountVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function stemming_tokenizer at 0x7f83a113d290>)),\n",
              "                ('classifier', MultinomialNB(alpha=0.01))])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.2. Support Vector Classification con stochatic gradient descent"
      ],
      "metadata": {
        "id": "zrmKvjXsxGgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for alpha in [1, 0.1, 0.01, 0.001, 0.0001, 0.00001]:  \n",
        "  SVMclf1 = Pipeline(\n",
        "      [('vectorizer',CountVectorizer(stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "      ('classifier', SGDClassifier(alpha=alpha))]\n",
        "  )\n",
        "\n",
        "  train_fn(SVMclf1, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U6GJl9gxCxv",
        "outputId": "a6fd44c1-2b58-47c6-d6db-d07bb58bb6e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 52.78% - Time duration: 4.49 s\n",
            "Test accuracy: 63.30% - Time duration: 5.04 s\n",
            "Test accuracy: 73.98% - Time duration: 4.75 s\n",
            "Test accuracy: 69.71% - Time duration: 5.25 s\n",
            "Test accuracy: 66.20% - Time duration: 5.95 s\n",
            "Test accuracy: 65.90% - Time duration: 6.57 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SVMclf1 = Pipeline(\n",
        "    [('vectorizer',CountVectorizer(tokenizer=stemming_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', SGDClassifier(alpha=0.01))]\n",
        "  )\n",
        "\n",
        "train_fn(SVMclf1, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLTYgG01xOxe",
        "outputId": "5ff6ec63-a2a2-4f84-811f-51a9a6986b50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 70.69% - Time duration: 86.73 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 CountVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function stemming_tokenizer at 0x7f83a113d290>)),\n",
              "                ('classifier', SGDClassifier(alpha=0.01))])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "58sTITBAqZcK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}