{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWjooeTYAnTf"
      },
      "source": [
        "# Notebook probar diferentes clasificadores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcIrGa9gAunO"
      },
      "source": [
        "Se pretende explorar diferentes clasificadores variando los hyperparametros, escogiendo el mejor resultado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjDjZObJA64d"
      },
      "source": [
        "### 1. Importación librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AB-3fopyAdC2"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.naive_bayes import  MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier, Perceptron, LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "import re\n",
        "import time\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-SGSteDFvoA"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('popular')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sjhvhB_wHYWO"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
        "from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKRgkTfcBOcI",
        "outputId": "90e4d544-e96a-492d-c7e8-e3ec19a5dd77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de articulos: 18846\n",
            "Número de categorias: 20\n"
          ]
        }
      ],
      "source": [
        "news = fetch_20newsgroups(subset=\"all\",remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "print(\"Número de articulos: {}\".format(len(news.data)))\n",
        "print(\"Número de categorias: {}\".format(len(news.target_names)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmVicDeMB-q7"
      },
      "source": [
        "### 2. Definción función de entrenamiento  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PjNpzoH2B-Hx"
      },
      "outputs": [],
      "source": [
        "def train_fn(classifier, X, Y):\n",
        "  start = time.time()\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
        "  classifier.fit(X_train, Y_train)\n",
        "  end = time.time()\n",
        "  score = classifier.score(X_test, Y_test)\n",
        "  print(\"Test accuracy: {:.2f}% - Time duration: {:.2f} s\".format(score * 100, (end-start)))\n",
        "  return classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PXKQREVHiwn"
      },
      "source": [
        "### 3. Preprocesado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkut_9knpwva"
      },
      "source": [
        "#### 3.1. Limpiar el texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RPULiDtNpvJ3"
      },
      "outputs": [],
      "source": [
        "def clean_text(news):\n",
        "\n",
        "  df = pd.DataFrame(news.data, columns=['text'])\n",
        "  df[\"categories\"] = [news.target_names[i] for i in news.target]\n",
        "  df[\"labels\"] = [i for i in news.target]\n",
        "\n",
        "  signos = '[\\\\!\\\\\"\\\\#\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\-\\\\.\\\\/\\\\:\\\\;\\\\<\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\^_\\\\`\\\\{\\\\|\\\\}\\\\~]'\n",
        "  # Remove signo puntuacion \n",
        "  df[\"text\"] = [re.sub(signos, ' ', sent) for sent in df[\"text\"]]\n",
        "  # Remove whitspace star and end text\n",
        "  df['text'] = df['text'].str.strip()\n",
        "  # Remove Emails\n",
        "  df[\"text\"] = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in df[\"text\"]]\n",
        "  # Remove distracting single quotes\n",
        "  df[\"text\"] = [re.sub(\"\\'\", \"\", sent) for sent in df[\"text\"]]\n",
        "  # Remove any rows with empty fields\n",
        "  df = df.replace('', np.NaN).dropna()\n",
        "  # Drop duplicates\n",
        "  df = df.drop_duplicates(subset='text')\n",
        "  # Remove number\n",
        "  df['text'] = [re.sub('\\d+', ' ', text) for text in df['text']]\n",
        "  # lower\n",
        "  df[\"text\"] = [text.lower() for text in df[\"text\"]]\n",
        "  # Remove multiples whitspaces characters\n",
        "  df[\"text\"] = [re.sub('\\s+', ' ', sent) for sent in df[\"text\"]]\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZd4fKtwp26b"
      },
      "source": [
        "#### 3.2. Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FGu3xG88Hh96"
      },
      "outputs": [],
      "source": [
        "def stemming_tokenizer(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(w) for w in word_tokenize(text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y96NX05wgR1q"
      },
      "source": [
        "#### 3.3. Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RKGi6rnBgRUR"
      },
      "outputs": [],
      "source": [
        "#defining the function for lemmatization\n",
        "def lemmatizer_tokenizer(text):\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    return [wordnet_lemmatizer.lemmatize(w) for w in word_tokenize(text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Fet__Q76qGqX"
      },
      "outputs": [],
      "source": [
        "df = clean_text(news)\n",
        "data = df[\"text\"].values\n",
        "target = df[\"labels\"].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBdlxOaPEPD6"
      },
      "source": [
        "### 4. Construcción de clasificadores - TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF8aicupEXEP"
      },
      "source": [
        "#### 4.1. Multinominal Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JesfWH5YBxKc",
        "outputId": "4436501b-0123-4ef3-c843-cca7ffb2c7bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 73.04% - Time duration: 2.89 s\n",
            "Test accuracy: 77.34% - Time duration: 3.00 s\n",
            "Test accuracy: 78.36% - Time duration: 2.62 s\n",
            "Test accuracy: 77.70% - Time duration: 2.45 s\n",
            "Test accuracy: 75.18% - Time duration: 4.16 s\n"
          ]
        }
      ],
      "source": [
        "for alpha in [1, 0.1, 0.01, 0.001, 0.0001]:\n",
        "  NBclf = Pipeline(\n",
        "      [('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english')+ list(string.punctuation))),\n",
        "      ('classifier', MultinomialNB(alpha=alpha))]\n",
        "  )\n",
        "\n",
        "  train_fn(NBclf, data, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y73u9n6IOzR"
      },
      "source": [
        "Se escoge el mejor desempeño y se realiza prueba eliminado signo de puntuación y stemming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brZd2_m6FHQO",
        "outputId": "9016342f-0fd0-48a7-8451-067f0a085c78"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Maycol\\env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 77.84% - Time duration: 76.04 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function stemming_tokenizer at 0x0000021466D634C0>)),\n",
              "                ('classifier', MultinomialNB(alpha=0.01))])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "NBclf = Pipeline(\n",
        "    [('vectorizer', TfidfVectorizer(tokenizer=stemming_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))),\n",
        "    ('classifier', MultinomialNB(alpha=0.01))]\n",
        ")\n",
        "train_fn(NBclf, data, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nq6ufPteg9ND",
        "outputId": "bfdd6a3b-3059-4c3a-d165-5c104711742b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Maycol\\env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 78.99% - Time duration: 32.84 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function lemmatizer_tokenizer at 0x0000021466D63550>)),\n",
              "                ('classifier', MultinomialNB(alpha=0.01))])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "NBclf = Pipeline(\n",
        "    [('vectorizer', TfidfVectorizer(tokenizer=lemmatizer_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))),\n",
        "    ('classifier', MultinomialNB(alpha=0.01))]\n",
        ")\n",
        "train_fn(NBclf, data, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f05ulzFmJHKX"
      },
      "source": [
        "#### 4.2. Support Vector Classification con stochatic gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJZfEqcSIuYK",
        "outputId": "70f07a9a-0357-418c-b70d-124970473801"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 10.55% - Time duration: 4.39 s\n",
            "Test accuracy: 59.48% - Time duration: 5.24 s\n",
            "Test accuracy: 75.32% - Time duration: 3.26 s\n",
            "Test accuracy: 74.11% - Time duration: 3.31 s\n",
            "Test accuracy: 78.27% - Time duration: 4.27 s\n",
            "Test accuracy: 75.84% - Time duration: 3.66 s\n"
          ]
        }
      ],
      "source": [
        "for alpha in [1, 0.1, 0.01, 0.001, 0.0001, 0.00001]:  \n",
        "  SVMclf1 = Pipeline(\n",
        "      [('vectorizer',TfidfVectorizer(stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "      ('classifier', SGDClassifier(alpha=alpha))]\n",
        "  )\n",
        "\n",
        "  train_fn(SVMclf1, data, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThuTi9rHNYPE"
      },
      "source": [
        "Se escoge el mejor desempeño y se realiza prueba eliminado signo de puntuación y stemming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcpMNTR0J0Bp",
        "outputId": "51f3f809-e974-41d1-908c-423489509ab8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Maycol\\env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 76.74% - Time duration: 77.23 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function stemming_tokenizer at 0x0000021466D634C0>)),\n",
              "                ('classifier', SGDClassifier())])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SVMclf1 = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(tokenizer=stemming_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', SGDClassifier(alpha=0.0001))]\n",
        "  )\n",
        "\n",
        "train_fn(SVMclf1, data, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgCX-FKqhgAd",
        "outputId": "dc02c0f0-fff9-4959-9c62-cf707b6631e6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Maycol\\env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 77.40% - Time duration: 32.61 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function lemmatizer_tokenizer at 0x0000021466D63550>)),\n",
              "                ('classifier', SGDClassifier())])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SVMclf1 = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(tokenizer=lemmatizer_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', SGDClassifier(alpha=0.0001))]\n",
        "  )\n",
        "\n",
        "train_fn(SVMclf1, data, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSxjUK-tNaK6"
      },
      "source": [
        "#### 4.3. Support Vector Classification con liner SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0PWzhVGME42",
        "outputId": "d1bdaf6b-2d55-44df-d592-7c316518e35a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 77.07% - Time duration: 4.30 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
              "                ('classifier', LinearSVC())])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SVMclf2 = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', LinearSVC())]\n",
        ")\n",
        "\n",
        "train_fn(SVMclf2, data, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvQiI8ukOY5z"
      },
      "source": [
        "Con stemming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTAYkT3FOYa1",
        "outputId": "29be9623-87a5-4971-b300-4aa72e305d85"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Maycol\\env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 77.37% - Time duration: 75.60 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function stemming_tokenizer at 0x0000021466D634C0>)),\n",
              "                ('classifier', LinearSVC())])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SVMclf2 = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(tokenizer=stemming_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', LinearSVC())]\n",
        ")\n",
        "\n",
        "train_fn(SVMclf2, data, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7E-WG3tMhrpb",
        "outputId": "37e53a39-ec64-4007-fb1c-100366e5858e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Maycol\\env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 77.45% - Time duration: 31.56 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function lemmatizer_tokenizer at 0x0000021466D63550>)),\n",
              "                ('classifier', LinearSVC())])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SVMclf2 = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(tokenizer=lemmatizer_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', LinearSVC())]\n",
        ")\n",
        "\n",
        "train_fn(SVMclf2, data, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVouH3sqRwni"
      },
      "source": [
        "#### 4.4. Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AM188sjFOMK6",
        "outputId": "95f75d02-cfed-4bcf-d94e-7214111f54ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 71.12% - Time duration: 3.46 s\n",
            "Test accuracy: 70.77% - Time duration: 3.00 s\n",
            "Test accuracy: 71.23% - Time duration: 3.20 s\n",
            "Test accuracy: 71.95% - Time duration: 3.11 s\n",
            "Test accuracy: 71.23% - Time duration: 3.11 s\n",
            "Test accuracy: 71.04% - Time duration: 3.31 s\n"
          ]
        }
      ],
      "source": [
        "for alpha in [1, 0.1, 0.01, 0.001, 0.0001, 0.00001]:  \n",
        "  perclf = Pipeline(\n",
        "      [('vectorizer',TfidfVectorizer(stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "      ('classifier', Perceptron(alpha=alpha))]\n",
        "  )\n",
        "\n",
        "  train_fn(perclf, data, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ijh0cvY6SE0w",
        "outputId": "ca390bdf-66f0-4d21-e1bd-0b4f912c4049"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Maycol\\env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 71.45% - Time duration: 71.30 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function stemming_tokenizer at 0x0000021466D634C0>)),\n",
              "                ('classifier', Perceptron(alpha=0.001))])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "perclf = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(tokenizer=stemming_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', Perceptron(alpha=0.001))]\n",
        ")\n",
        "\n",
        "train_fn(perclf, data, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8fo_KbRh-vg",
        "outputId": "9840a69f-10b9-491a-8867-e805639f64b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Maycol\\env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 72.03% - Time duration: 30.27 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function lemmatizer_tokenizer at 0x0000021466D63550>)),\n",
              "                ('classifier', Perceptron(alpha=1e-05))])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "perclf = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(tokenizer=lemmatizer_tokenizer, stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', Perceptron(alpha=0.00001))]\n",
        ")\n",
        "\n",
        "train_fn(perclf, data, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZU9cUHku73C"
      },
      "source": [
        "#### 4.5. Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGG3e9GxSVYz",
        "outputId": "97eca174-0212-4bf6-e02b-e90ab302bb59"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Maycol\\env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 76.79% - Time duration: 107.58 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function stemming_tokenizer at 0x0000021466D634C0>)),\n",
              "                ('classifier', LogisticRegression(max_iter=10000))])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "perclf = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(tokenizer=stemming_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', LogisticRegression(max_iter=10000))]\n",
        ")\n",
        "\n",
        "train_fn(perclf, data, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfHFU_T1jpEW",
        "outputId": "133c9040-4a0f-4125-cbdd-fb89fd0e1541"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Maycol\\env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 75.62% - Time duration: 68.77 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function lemmatizer_tokenizer at 0x0000021466D63550>)),\n",
              "                ('classifier', LogisticRegression(max_iter=10000))])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "perclf = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(tokenizer=lemmatizer_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', LogisticRegression(max_iter=10000))]\n",
        ")\n",
        "\n",
        "train_fn(perclf, data, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7pkA37NwMGu"
      },
      "source": [
        "### 5. Construcción de clasificadores - CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-LbcMunwuNj"
      },
      "source": [
        "#### 5.1. Multinominal Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljZhvSbVvnZo",
        "outputId": "e3264553-1f45-4599-ff15-860bed4c7986"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 70.25% - Time duration: 2.27 s\n",
            "Test accuracy: 73.04% - Time duration: 2.22 s\n",
            "Test accuracy: 74.19% - Time duration: 2.48 s\n",
            "Test accuracy: 73.15% - Time duration: 2.35 s\n",
            "Test accuracy: 72.82% - Time duration: 2.33 s\n"
          ]
        }
      ],
      "source": [
        "for alpha in [1, 0.1, 0.01, 0.001, 0.0001]:\n",
        "  NBclf = Pipeline(\n",
        "      [('vectorizer', CountVectorizer(stop_words=stopwords.words('english') + list(string.punctuation))),\n",
        "      ('classifier', MultinomialNB(alpha=alpha))]\n",
        "  )\n",
        "\n",
        "  train_fn(NBclf, data, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXs-YGggw2Eb",
        "outputId": "d2dcce86-a04f-4a04-e130-51fb29e76f76"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Maycol\\env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 73.01% - Time duration: 62.91 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 CountVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function stemming_tokenizer at 0x0000021466D634C0>)),\n",
              "                ('classifier', MultinomialNB(alpha=0.01))])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "NBclf = Pipeline(\n",
        "    [('vectorizer', CountVectorizer(tokenizer=stemming_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))),\n",
        "    ('classifier', MultinomialNB(alpha=0.01))]\n",
        ")\n",
        "train_fn(NBclf, data, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrmKvjXsxGgP"
      },
      "source": [
        "#### 5.2. Support Vector Classification con stochatic gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U6GJl9gxCxv",
        "outputId": "a6fd44c1-2b58-47c6-d6db-d07bb58bb6e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 51.92% - Time duration: 3.30 s\n",
            "Test accuracy: 64.08% - Time duration: 3.78 s\n",
            "Test accuracy: 73.45% - Time duration: 3.61 s\n",
            "Test accuracy: 71.12% - Time duration: 3.83 s\n",
            "Test accuracy: 64.16% - Time duration: 4.64 s\n",
            "Test accuracy: 64.49% - Time duration: 6.11 s\n"
          ]
        }
      ],
      "source": [
        "for alpha in [1, 0.1, 0.01, 0.001, 0.0001, 0.00001]:  \n",
        "  SVMclf1 = Pipeline(\n",
        "      [('vectorizer',CountVectorizer(stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "      ('classifier', SGDClassifier(alpha=alpha))]\n",
        "  )\n",
        "\n",
        "  train_fn(SVMclf1, data, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLTYgG01xOxe",
        "outputId": "5ff6ec63-a2a2-4f84-811f-51a9a6986b50"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Maycol\\env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 73.37% - Time duration: 71.38 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 CountVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function stemming_tokenizer at 0x0000021466D634C0>)),\n",
              "                ('classifier', SGDClassifier(alpha=0.01))])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SVMclf1 = Pipeline(\n",
        "    [('vectorizer',CountVectorizer(tokenizer=stemming_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', SGDClassifier(alpha=0.01))]\n",
        "  )\n",
        "\n",
        "train_fn(SVMclf1, data, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58sTITBAqZcK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "text-classifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
