{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Several- Classifiers.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPfwQQFZNuEYV31L9lWbVMU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/santiagoruiz-udea/News_classification/blob/main/Several_Classifiers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook probar diferentes clasificadores"
      ],
      "metadata": {
        "id": "RWjooeTYAnTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se pretende explorar diferentes clasificadores variando los hyperparametros, escogiendo el mejor resultado."
      ],
      "metadata": {
        "id": "KcIrGa9gAunO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Importación librerias"
      ],
      "metadata": {
        "id": "EjDjZObJA64d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AB-3fopyAdC2"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.naive_bayes import  MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier, Perceptron, LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "import re\n",
        "import time\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('popular')"
      ],
      "metadata": {
        "id": "U-SGSteDFvoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
        "from nltk import word_tokenize"
      ],
      "metadata": {
        "id": "sjhvhB_wHYWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories = [\n",
        "    \"comp.graphics\",\n",
        "    \"misc.forsale\",\n",
        "    \"rec.autos\",\n",
        "    \"rec.motorcycles\",\n",
        "    \"rec.sport.baseball\",\n",
        "    \"sci.electronics\",\n",
        "    \"sci.space\",\n",
        "    \"talk.religion.misc\",\n",
        "    \"talk.politics.misc\",\n",
        "    \"alt.atheism\",\n",
        "    \"comp.sys.ibm.pc.hardware\",\n",
        "    \"rec.sport.hockey\",\n",
        "    \"sci.crypt\",\n",
        "    \"sci.med\",\n",
        "    \"talk.politics.guns\",\n",
        "]\n",
        "news = fetch_20newsgroups(subset=\"all\", categories=categories,remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=42)\n",
        "\n",
        "print(\"Número de articulos: {}\".format(len(news.data)))\n",
        "print(\"Número de categorias: {}\".format(len(news.target_names)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKRgkTfcBOcI",
        "outputId": "08506515-9500-40fd-cb0e-4e558646d976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de articulos: 13973\n",
            "Número de categorias: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Definción función de entrenamiento  "
      ],
      "metadata": {
        "id": "MmVicDeMB-q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(classifier, X, Y):\n",
        "  start = time.time()\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
        "  classifier.fit(X_train, Y_train)\n",
        "  end = time.time()\n",
        "  score = classifier.score(X_test, Y_test)\n",
        "  print(\"Test accuracy: {:.2f}% - Time duration: {:.2f} s\".format(score * 100, (end-start)))\n",
        "  return classifier"
      ],
      "metadata": {
        "id": "PjNpzoH2B-Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Preprocesado"
      ],
      "metadata": {
        "id": "7PXKQREVHiwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Limpiar el texto"
      ],
      "metadata": {
        "id": "rkut_9knpwva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(news):\n",
        "\n",
        "  df = pd.DataFrame(news.data, columns=['text'])\n",
        "  df[\"categories\"] = [news.target_names[i] for i in news.target]\n",
        "  df[\"labels\"] = [i for i in news.target]\n",
        "\n",
        "  # Remove multiples whitspaces characters\n",
        "  df[\"text\"] = [re.sub('\\s+', ' ', sent) for sent in df[\"text\"]]\n",
        "  # Remove whitspace star and end text\n",
        "  df['text'] = df['text'].str.strip()\n",
        "  # Remove Emails\n",
        "  df[\"text\"] = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in df[\"text\"]]\n",
        "  # Remove distracting single quotes\n",
        "  df[\"text\"] = [re.sub(\"\\'\", \"\", sent) for sent in df[\"text\"]]\n",
        "  # Remove any rows with empty fields\n",
        "  df = df.replace('', np.NaN).dropna()\n",
        "  # Drop duplicates\n",
        "  df = df.drop_duplicates(subset='text')\n",
        "  # Remove number\n",
        "  df['text'] = [re.sub(r'\\[[0-9]*\\]', ' ', text) for text in df['text']]\n",
        "  # lower\n",
        "  df[\"text\"] = [text.lower() for text in df[\"text\"]]\n",
        "  # Remove multiples whitspaces characters\n",
        "  df[\"text\"] = [re.sub('\\s+', ' ', sent) for sent in df[\"text\"]]\n",
        "  return df"
      ],
      "metadata": {
        "id": "RPULiDtNpvJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. Stemming"
      ],
      "metadata": {
        "id": "NZd4fKtwp26b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stemming_tokenizer(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(w) for w in word_tokenize(text)]"
      ],
      "metadata": {
        "id": "FGu3xG88Hh96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Lemmatization"
      ],
      "metadata": {
        "id": "Y96NX05wgR1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the function for lemmatization\n",
        "def lemmatizer_tokenizer(text):\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    return [wordnet_lemmatizer.lemmatize(w) for w in word_tokenize(text)]"
      ],
      "metadata": {
        "id": "RKGi6rnBgRUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = clean_text(news)\n",
        "data = df[\"text\"].values\n",
        "target = df[\"labels\"].values"
      ],
      "metadata": {
        "id": "Fet__Q76qGqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Construcción de clasificadores - TfidfVectorizer"
      ],
      "metadata": {
        "id": "IBdlxOaPEPD6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Multinominal Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "iF8aicupEXEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for alpha in [1, 0.1, 0.01, 0.001, 0.0001]:\n",
        "  NBclf = Pipeline(\n",
        "      [('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english')+ list(string.punctuation))),\n",
        "      ('classifier', MultinomialNB(alpha=alpha))]\n",
        "  )\n",
        "\n",
        "  train_fn(NBclf, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JesfWH5YBxKc",
        "outputId": "abfb2f94-aefe-4083-f6ed-3dd38b0b6acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 77.33% - Time duration: 1.71 s\n",
            "Test accuracy: 80.02% - Time duration: 1.81 s\n",
            "Test accuracy: 81.28% - Time duration: 1.73 s\n",
            "Test accuracy: 79.10% - Time duration: 1.75 s\n",
            "Test accuracy: 78.43% - Time duration: 1.78 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se escoge el mejor desempeño y se realiza prueba eliminado signo de puntuación y stemming."
      ],
      "metadata": {
        "id": "0Y73u9n6IOzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NBclf = Pipeline(\n",
        "    [('vectorizer', TfidfVectorizer(tokenizer=stemming_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))),\n",
        "    ('classifier', MultinomialNB(alpha=0.01))]\n",
        ")\n",
        "train_fn(NBclf, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brZd2_m6FHQO",
        "outputId": "4295e906-6402-4963-dfd4-14b2582ef0f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 81.39% - Time duration: 58.29 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function stemming_tokenizer at 0x7f3b39ebab90>)),\n",
              "                ('classifier', MultinomialNB(alpha=0.01))])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se escoge el mejor desempeño y se realiza prueba eliminado signo de puntuación y Lemmatizer."
      ],
      "metadata": {
        "id": "3sPvu-ft7BIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NBclf = Pipeline(\n",
        "    [('vectorizer', TfidfVectorizer(tokenizer=lemmatizer_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))),\n",
        "    ('classifier', MultinomialNB(alpha=0.01))]\n",
        ")\n",
        "train_fn(NBclf, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nq6ufPteg9ND",
        "outputId": "c243e101-553f-448e-b714-061b3e87e39e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 82.02% - Time duration: 28.96 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function lemmatizer_tokenizer at 0x7f3b39e85b00>)),\n",
              "                ('classifier', MultinomialNB(alpha=0.01))])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. Support Vector Classification con stochatic gradient descent"
      ],
      "metadata": {
        "id": "f05ulzFmJHKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for alpha in [1, 0.1, 0.01, 0.001, 0.0001, 0.00001]:  \n",
        "  SVMclf1 = Pipeline(\n",
        "      [('vectorizer',TfidfVectorizer(stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "      ('classifier', SGDClassifier(alpha=alpha))]\n",
        "  )\n",
        "\n",
        "  train_fn(SVMclf1, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJZfEqcSIuYK",
        "outputId": "a31d4a8b-30d9-49af-c46c-d0b81e0ba8ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 6.46% - Time duration: 3.25 s\n",
            "Test accuracy: 57.50% - Time duration: 4.22 s\n",
            "Test accuracy: 77.70% - Time duration: 3.04 s\n",
            "Test accuracy: 76.55% - Time duration: 2.23 s\n",
            "Test accuracy: 79.95% - Time duration: 2.60 s\n",
            "Test accuracy: 79.43% - Time duration: 2.49 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se escoge el mejor desempeño y se realiza prueba eliminado signo de puntuación y stemming."
      ],
      "metadata": {
        "id": "ThuTi9rHNYPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SVMclf1 = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(tokenizer=stemming_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', SGDClassifier(alpha=0.0001))]\n",
        "  )\n",
        "\n",
        "train_fn(SVMclf1, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcpMNTR0J0Bp",
        "outputId": "5470f891-24d6-4783-a1f8-37cee58be504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 78.95% - Time duration: 56.57 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function stemming_tokenizer at 0x7f3b39ebab90>)),\n",
              "                ('classifier', SGDClassifier())])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SVMclf1 = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(tokenizer=lemmatizer_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', SGDClassifier(alpha=0.0001))]\n",
        "  )\n",
        "\n",
        "train_fn(SVMclf1, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgCX-FKqhgAd",
        "outputId": "862a12c2-729b-4e74-b6cb-b95883e608a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 79.58% - Time duration: 28.81 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function lemmatizer_tokenizer at 0x7f3b39e85b00>)),\n",
              "                ('classifier', SGDClassifier())])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3. Support Vector Classification con liner SVC"
      ],
      "metadata": {
        "id": "CSxjUK-tNaK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SVMclf2 = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', LinearSVC())]\n",
        ")\n",
        "\n",
        "train_fn(SVMclf2, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0PWzhVGME42",
        "outputId": "c5ee3da2-c378-4255-986a-38749ae42fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 81.17% - Time duration: 2.73 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
              "                ('classifier', LinearSVC())])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con stemming."
      ],
      "metadata": {
        "id": "vvQiI8ukOY5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SVMclf2 = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(tokenizer=stemming_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', LinearSVC())]\n",
        ")\n",
        "\n",
        "train_fn(SVMclf2, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTAYkT3FOYa1",
        "outputId": "d687a002-d8dd-407c-f632-b1445ec3e422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 79.69% - Time duration: 56.67 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function stemming_tokenizer at 0x7f3b39ebab90>)),\n",
              "                ('classifier', LinearSVC())])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SVMclf2 = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(tokenizer=lemmatizer_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', LinearSVC())]\n",
        ")\n",
        "\n",
        "train_fn(SVMclf2, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7E-WG3tMhrpb",
        "outputId": "64416c2e-bcf5-4c3b-d152-759cfd0223f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 79.39% - Time duration: 28.68 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function lemmatizer_tokenizer at 0x7f3b39e85b00>)),\n",
              "                ('classifier', LinearSVC())])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4. Perceptron"
      ],
      "metadata": {
        "id": "JVouH3sqRwni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for alpha in [1, 0.1, 0.01, 0.001, 0.0001, 0.00001]:  \n",
        "  perclf = Pipeline(\n",
        "      [('vectorizer',TfidfVectorizer(stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "      ('classifier', Perceptron(alpha=alpha))]\n",
        "  )\n",
        "\n",
        "  train_fn(perclf, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AM188sjFOMK6",
        "outputId": "0d000ebe-7dd5-4d38-8d4a-b3b46024085a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 73.38% - Time duration: 2.27 s\n",
            "Test accuracy: 74.22% - Time duration: 2.28 s\n",
            "Test accuracy: 75.37% - Time duration: 2.22 s\n",
            "Test accuracy: 76.11% - Time duration: 2.26 s\n",
            "Test accuracy: 74.22% - Time duration: 2.16 s\n",
            "Test accuracy: 74.08% - Time duration: 2.21 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perclf = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(tokenizer=stemming_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', Perceptron(alpha=0.001))]\n",
        ")\n",
        "\n",
        "train_fn(perclf, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ijh0cvY6SE0w",
        "outputId": "08bb3cbc-90e2-4f5c-97c0-845a177eb00a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 75.15% - Time duration: 60.63 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function stemming_tokenizer at 0x7f3b39ebab90>)),\n",
              "                ('classifier', Perceptron(alpha=0.001))])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perclf = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(tokenizer=lemmatizer_tokenizer, stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', Perceptron(alpha=0.001))]\n",
        ")\n",
        "\n",
        "train_fn(perclf, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8fo_KbRh-vg",
        "outputId": "fdf8366c-9eff-4902-f0e1-fe34b0481b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 75.59% - Time duration: 27.78 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function lemmatizer_tokenizer at 0x7f3b39e85b00>)),\n",
              "                ('classifier', Perceptron(alpha=0.001))])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5. Logistic Regression"
      ],
      "metadata": {
        "id": "ZZU9cUHku73C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perclf = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(tokenizer=stemming_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', LogisticRegression(max_iter=10000))]\n",
        ")\n",
        "\n",
        "train_fn(perclf, data, target)"
      ],
      "metadata": {
        "id": "kGG3e9GxSVYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e13a4f3-f199-4f77-a04f-e06d20288ff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 77.51% - Time duration: 91.72 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function stemming_tokenizer at 0x7f3b39ebab90>)),\n",
              "                ('classifier', LogisticRegression(max_iter=10000))])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perclf = Pipeline(\n",
        "    [('vectorizer',TfidfVectorizer(tokenizer=lemmatizer_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', LogisticRegression(max_iter=10000))]\n",
        ")\n",
        "\n",
        "train_fn(perclf, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfHFU_T1jpEW",
        "outputId": "c9f5bd3e-47ef-43c3-b773-a01ff12563e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 76.44% - Time duration: 64.15 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function lemmatizer_tokenizer at 0x7f3b39e85b00>)),\n",
              "                ('classifier', LogisticRegression(max_iter=10000))])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Construcción de clasificadores - CountVectorizer"
      ],
      "metadata": {
        "id": "a7pkA37NwMGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1. Multinominal Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "d-LbcMunwuNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for alpha in [1, 0.1, 0.01, 0.001, 0.0001]:\n",
        "  NBclf = Pipeline(\n",
        "      [('vectorizer', CountVectorizer(stop_words=stopwords.words('english') + list(string.punctuation))),\n",
        "      ('classifier', MultinomialNB(alpha=alpha))]\n",
        "  )\n",
        "\n",
        "  train_fn(NBclf, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljZhvSbVvnZo",
        "outputId": "73373327-1ced-4b76-8843-986f2ead3d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 76.74% - Time duration: 1.68 s\n",
            "Test accuracy: 78.47% - Time duration: 1.67 s\n",
            "Test accuracy: 78.14% - Time duration: 1.71 s\n",
            "Test accuracy: 76.88% - Time duration: 1.76 s\n",
            "Test accuracy: 76.77% - Time duration: 1.67 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NBclf = Pipeline(\n",
        "    [('vectorizer', CountVectorizer(tokenizer=stemming_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))),\n",
        "    ('classifier', MultinomialNB(alpha=0.01))]\n",
        ")\n",
        "train_fn(NBclf, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXs-YGggw2Eb",
        "outputId": "03286cf7-b0f7-4622-b75f-a4ec7a53ff5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 77.51% - Time duration: 56.54 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 CountVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function stemming_tokenizer at 0x7f3b39ebab90>)),\n",
              "                ('classifier', MultinomialNB(alpha=0.01))])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2. Support Vector Classification con stochatic gradient descent"
      ],
      "metadata": {
        "id": "zrmKvjXsxGgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for alpha in [1, 0.1, 0.01, 0.001, 0.0001, 0.00001]:  \n",
        "  SVMclf1 = Pipeline(\n",
        "      [('vectorizer',CountVectorizer(stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "      ('classifier', SGDClassifier(alpha=alpha))]\n",
        "  )\n",
        "\n",
        "  train_fn(SVMclf1, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U6GJl9gxCxv",
        "outputId": "105b4254-fd30-44a2-af47-9819e8ae9a2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 59.16% - Time duration: 2.60 s\n",
            "Test accuracy: 66.77% - Time duration: 2.83 s\n",
            "Test accuracy: 75.44% - Time duration: 2.58 s\n",
            "Test accuracy: 74.15% - Time duration: 2.99 s\n",
            "Test accuracy: 69.09% - Time duration: 3.19 s\n",
            "Test accuracy: 68.76% - Time duration: 3.19 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SVMclf1 = Pipeline(\n",
        "    [('vectorizer',CountVectorizer(tokenizer=stemming_tokenizer,stop_words=stopwords.words('english') + list(string.punctuation))), \n",
        "    ('classifier', SGDClassifier(alpha=0.01))]\n",
        "  )\n",
        "\n",
        "train_fn(SVMclf1, data, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLTYgG01xOxe",
        "outputId": "cd5fec5a-6453-47b3-81f4-75929ffa2cb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 70.24% - Time duration: 57.33 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 CountVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you',\n",
              "                                             \"you're\", \"you've\", \"you'll\",\n",
              "                                             \"you'd\", 'your', 'yours',\n",
              "                                             'yourself', 'yourselves', 'he',\n",
              "                                             'him', 'his', 'himself', 'she',\n",
              "                                             \"she's\", 'her', 'hers', 'herself',\n",
              "                                             'it', \"it's\", 'its', 'itself', ...],\n",
              "                                 tokenizer=<function stemming_tokenizer at 0x7f3b39ebab90>)),\n",
              "                ('classifier', SGDClassifier(alpha=0.01))])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "58sTITBAqZcK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}